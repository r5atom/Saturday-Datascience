{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdw_top'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add extra information from RDW open data\n",
    "\n",
    "Query to the open data dataset of the RDW.\n",
    "\n",
    "\n",
    "1. <a href=\"#rdw_registrations\">Registration numbers</a>  \n",
    "    Apis with registration as key\n",
    "2. <a href=\"#rdw_confcodes\">Conformity codes</a>  \n",
    "    Cars get a conformity code when certified.\n",
    "3. <a href=\"#rdw_other_apis\">Other APIs</a>  \n",
    "    It may take a while (10 min) to query all conformity codes individually.\n",
    "4. <a href=\"#rdw_ovi\">Closed data</a>  \n",
    "    Get closed data from RDW website. This also takes a while because of time out enforced by website. Use config to disable.\n",
    "5. <a href=\"#rdw_merge\">Merge results</a>  \n",
    "    Combine all dataframes and save\n",
    "6. <a href=\"#rdw_save\">Save results</a>  \n",
    "- - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# setting path\n",
    "os.chdir(r'..')\n",
    "\n",
    "import drz_config\n",
    "cfg = drz_config.read_config()\n",
    "DATE = cfg['DATE']\n",
    "VERBOSE = cfg['VERBOSE']\n",
    "OPBOD = cfg['OPBOD']\n",
    "CLOSEDDATA = cfg['CLOSEDDATA']\n",
    "closed_data_fields = cfg['closed_data_fields']\n",
    "SKIPSAVE = cfg['SKIPSAVE']\n",
    "if not OPBOD:\n",
    "    month_counter = cfg['URL'][-2:]\n",
    "else:\n",
    "    month_counter = cfg['URL'][-4:-2]\n",
    "\n",
    "QUICK_MERGE = False # check if rdw data already exist (ran when auction was still open)\n",
    "\n",
    "\n",
    "\n",
    "if VERBOSE > 0:\n",
    "    display(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import os\n",
    "# to keep api key hidden import this from sub dir\n",
    "import assets.hidden_api_keys as hidden_api_keys\n",
    "from time import sleep\n",
    "import urllib\n",
    "\n",
    "# base url\n",
    "apiurl = 'https://opendata.rdw.nl/resource/m9d7-ebf2.json?$$app_token=' + hidden_api_keys.socrata_apptoken + '&'\n",
    "\n",
    "def get_json_from_api(url,reg,c=0):\n",
    "    \n",
    "    '''Get json object from api'''\n",
    "    \n",
    "    import time\n",
    "\n",
    "    c+=1\n",
    "    try:\n",
    "        df=pd.read_json(url + 'kenteken=' + reg.replace('-','').upper()).to_dict()\n",
    "    except:\n",
    "        if c > 10:\n",
    "            print(url,reg)\n",
    "            raise \n",
    "        else:\n",
    "            print('pause 2 sec and try again!')\n",
    "            time.sleep(2)\n",
    "            df = get_json_from_api(url,reg,c)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "# get_json_from_api(apiurl,'61-sf-FG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load auction results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OPBOD = False\n",
    "NO_PRICE = False\n",
    "DATE = '2015-03'\n",
    "month_counter = ''\n",
    "file_name = f'../data/pdf-data-{DATE}.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "file_name = f'../data/auctions/results/drz-data-{DATE}-{month_counter}.pkl'\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "if not os.path.isfile(file_name):\n",
    "    # see if -without price- exists\n",
    "    NO_PRICE = True\n",
    "    if OPBOD:\n",
    "        file_name = file_name.replace('-opbod.pkl', '-opbod-without-price.pkl')\n",
    "    else:\n",
    "        file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "else:\n",
    "    NO_PRICE = False\n",
    "\n",
    "\n",
    "print(file_name)\n",
    "drz = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok continue running. Quick merge is: False\n"
     ]
    }
   ],
   "source": [
    "# load existing RDW data and merge with price\n",
    "if QUICK_MERGE:\n",
    "    file_name = f'../data/auctions/results/rdw-data-{DATE}-{month_counter}.pkl'\n",
    "    if os.path.isfile(file_name):\n",
    "        raise\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "    print(file_name)\n",
    "    rdw = pd.read_pickle(file_name)\n",
    "    \n",
    "    # add price\n",
    "    rdw.update(drz.Price)\n",
    "\n",
    "    # save\n",
    "    out = rdw.copy()\n",
    "    raise # is path correct?\n",
    "    file_name = file_name.replace('-without-price.pkl','.pkl')\n",
    "    if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "        print(file_name)\n",
    "        out.to_pickle(file_name)\n",
    "    else:\n",
    "        print(f'Skip. {file_name} exists or saving is disabled in settings.')\n",
    "        \n",
    "    assert False, \"Stop running. Everything is done\"\n",
    "else:\n",
    "    if VERBOSE: print('Ok continue running. Quick merge is:', QUICK_MERGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect registrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# see what lots have a Dutch registration (license number).\n",
    "hasReg = (~drz.Reg.isnull()) & (drz.Reg != 'onbekend') & (drz.Reg != '') & (~drz.LotType.isin([\n",
    "    'Vaartuig',\n",
    "    'Jetski',\n",
    "    'Sloep',\n",
    "    'Speedboot',\n",
    "    'Vaartuig (Type onbekend)',\n",
    "    'Motorvaartuig met opbouw (Pleziervaartuig)',\n",
    "]))\n",
    "\n",
    "print('nr. of registrations:',sum(hasReg))\n",
    "\n",
    "# adhoc fix\n",
    "idx = '2022-08-5012' # check in pictures. reg is wrong\n",
    "if idx in drz.index:\n",
    "    drz.loc[idx, 'Reg'] = 'LM-82-14'\n",
    "idx = '2022-29-5001' # check in pictures. reg is wrong\n",
    "if idx in drz.index:\n",
    "    drz.loc[idx, 'Reg'] = 'LM-82-14'\n",
    "idx = '2022-29-2008' # check in pictures. reg is wrong\n",
    "if idx in drz.index:\n",
    "    drz.loc[idx, 'Reg'] = 'KT-05-40'\n",
    "\n",
    "\n",
    "\n",
    "vc = drz.loc[hasReg, 'Reg'].str.upper().str.replace('-','').value_counts()\n",
    "if any(vc > 1):\n",
    "    display(vc[vc>1])\n",
    "    display(drz[drz.Reg.str.upper().str.replace('-','').isin(vc[vc>1].index)])\n",
    "    raise ValueError('Registration occurs in more than one lot.')\n",
    "# assert all(vc == 1), [, display(vc[vc>1])]\n",
    "\n",
    "# # make a copy and add info\n",
    "# rdw = drz.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_url(api_url, keys, token=hidden_api_keys.socrata_apptoken, query_field='kenteken'):\n",
    "    '''construct query url'''\n",
    "\n",
    "    import urllib\n",
    "\n",
    "    # First part\n",
    "    q = api_url + '?$$app_token=' + token + '&'\n",
    "\n",
    "    # convert list to string\n",
    "    id_list = ''.join([\"'{}', \".format(k) for k in keys])\n",
    "    id_list = id_list[0:-2] # remove trailing ', '\n",
    "    \n",
    "    # add escaped soql\n",
    "    soql = '$where=' + urllib.parse.quote(query_field + ' in (' + id_list + ')')\n",
    "    q += soql\n",
    "    \n",
    "    # See if field is available\n",
    "    accepted_fields = eval(urllib.request.urlopen(api_url).headers.get('X-SODA2-Fields'))\n",
    "    if query_field not in accepted_fields:\n",
    "        raise ValueError(f'<{query_field}> not allowed as SODA2 field in {api_url}.\\n\\tAccepted are: ' + ', '.join(accepted_fields))\n",
    "    \n",
    "    return q\n",
    "\n",
    "def long_to_wide(long, rank_col, index_name=None):\n",
    "    \n",
    "    '''\n",
    "    Convert dataframe to wide by appending rank\n",
    "    '''\n",
    "    \n",
    "    # input\n",
    "    if index_name is None:\n",
    "        index_name = long.index.name\n",
    "        \n",
    "    # ranking and append to index\n",
    "    rank_name = f'rank_{index_name}_x_{rank_col}'\n",
    "    long.loc[:, rank_name] = long.groupby(index_name)[rank_col].rank(method='first').astype(int)\n",
    "    long = long.reset_index().set_index([index_name, rank_name]).sort_index()\n",
    "    assert long.index.is_unique\n",
    "\n",
    "    # make wide and append rank to column name\n",
    "    wide = long.drop(columns=rank_col).unstack()\n",
    "    wide.columns = wide.columns.map('{0[0]}_{0[1]}'.format)\n",
    "    # add count\n",
    "    count_col = rank_name.replace('rank_', 'nr_of_')\n",
    "    wide.loc[:, count_col] = long.reset_index().groupby(index_name)[rank_name].max()\n",
    "\n",
    "    return wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of dataframes with different api results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# empty dictionary\n",
    "rdw_per_reg = dict()\n",
    "\n",
    "# first element of dict is registrations\n",
    "key = 'registrations'\n",
    "rdw_per_reg[key] = drz.loc[hasReg,['Reg', 'LotType']].copy() # copy from drz\n",
    "rdw_per_reg[key]['kenteken'] = rdw_per_reg[key].Reg.apply(lambda r: r.replace('-','').upper())\n",
    "rdw_per_reg[key].index.name = 'lot_index'\n",
    "rdw_per_reg[key] = rdw_per_reg[key].reset_index().set_index('kenteken')\n",
    "with pd.option_context('display.max_rows', 999):\n",
    "    display(rdw_per_reg[key].reset_index().set_index(['LotType', 'kenteken']).sort_index())\n",
    "\n",
    "print('\\n'.join(rdw_per_reg.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#rdw_top\" id='rdw_registrations'><font size=+1><center>^^ TOP ^^</center></font></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main api \n",
    "\n",
    "The main api: `api_gekentekende_voertuigen` points to subsequent apis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Assess these registrations\n",
    "regs = rdw_per_reg['registrations'].Reg.values\n",
    "regs = [r.replace('-','').upper() for r in regs]\n",
    "print(len(regs),'registrations in this set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Do main api first to get other possible apis\n",
    "api_name = 'api_gekentekende_voertuigen'\n",
    "api_url = 'https://opendata.rdw.nl/resource/m9d7-ebf2.json'\n",
    "\n",
    "# Query api\n",
    "key = re.sub('^api_','', api_name) # store with this key\n",
    "q = get_query_url(api_url,regs)\n",
    "rdw_per_reg[key] = pd.read_json(q)\n",
    "rdw_per_reg[key].set_index('kenteken', inplace=True)\n",
    "\n",
    "print(api_name, end=': ')\n",
    "if VERBOSE > 1:\n",
    "    display(rdw_per_reg[key])\n",
    "print(rdw_per_reg[key].shape)\n",
    "\n",
    "# Query other available apis\n",
    "for api_name in [c for c in rdw_per_reg['gekentekende_voertuigen'].columns if c.startswith('api')]:\n",
    "    print(api_name, end=': ')\n",
    "    key = re.sub('^api_','',api_name)\n",
    "    for api_url in rdw_per_reg['gekentekende_voertuigen'][api_name].unique():\n",
    "        print(api_url)\n",
    "        # query the web\n",
    "        q = get_query_url(api_url,regs)\n",
    "        df0 = pd.read_json(q)\n",
    "        # name of index\n",
    "        df0.columns.name = api_name\n",
    "\n",
    "        # query should return 'kenteken', make it the index\n",
    "        if df0.shape[0] != 0:\n",
    "            df0.set_index('kenteken', inplace=True)\n",
    "            \n",
    "            # Some apis return multiple values. Pivot around index number (\"volgnummer\")\n",
    "            if api_name == 'api_gekentekende_voertuigen_assen':\n",
    "                df0 = pd.pivot(df0, columns='as_nummer')\n",
    "\n",
    "            elif api_name == 'api_gekentekende_voertuigen_brandstof':\n",
    "                df0 = pd.pivot(df0, columns='brandstof_volgnummer')\n",
    "\n",
    "            elif api_name == 'api_gekentekende_voertuigen_carrosserie':\n",
    "                df0 = pd.pivot(df0, columns='carrosserie_volgnummer')\n",
    "\n",
    "            elif api_name == 'api_gekentekende_voertuigen_carrosserie_specifiek':\n",
    "                df0 = pd.pivot(df0, columns='carrosserie_voertuig_nummer_code_volgnummer')\n",
    "\n",
    "        # squeeze multi index\n",
    "        one_level = [\n",
    "            re.sub('^api_gekentekende_voertuigen_','',api_name) + '_' + '_'.join(\n",
    "                [str(c) if type(c)==int else c for c in l]\n",
    "            ) for l in df0.columns\n",
    "        ]\n",
    "        df0.columns = one_level\n",
    "\n",
    "        # add to list\n",
    "        if VERBOSE > 1:\n",
    "            display(df0.tail(3))\n",
    "        print(df0.shape)\n",
    "        rdw_per_reg[key]=df0\n",
    "        \n",
    "        \n",
    "print('\\n'.join(rdw_per_reg.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "file_name = f'../data/auctions/enriched-results/rdw-reg/rdw-reg-main-0-data-{DATE}-{month_counter}.pkl'\n",
    "\n",
    "out = pd.concat(rdw_per_reg, axis=1)\n",
    "out['TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')\n",
    "\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')\n",
    "    \n",
    "done_keys = list(rdw_per_reg.keys())\n",
    "done_keys.pop(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection api\n",
    "\n",
    "\"APK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Do main api first to get other possible apis\n",
    "api_name = 'api_meldingen_keuringsinstantie'\n",
    "api_url = 'https://opendata.rdw.nl/resource/sgfe-77wx.json'\n",
    "\n",
    "# Query api\n",
    "key = re.sub('^api_','', api_name) # store with this key\n",
    "q = get_query_url(api_url,regs)\n",
    "df0 = pd.read_json(q).add_prefix(key + '_')\n",
    "df0.set_index(key + '_kenteken', inplace=True)\n",
    "\n",
    "print(api_name, end=': ')\n",
    "print(api_url)\n",
    "\n",
    "long = df0.copy()\n",
    "long.loc[:, key + '_meld_datum_door_keuringsinstantie_dt_'] = pd.to_datetime(long.loc[:, key + '_meld_datum_door_keuringsinstantie_dt'])\n",
    "df0 = long_to_wide(long, key + '_meld_datum_door_keuringsinstantie_dt_')\n",
    "df0.rename(columns={'nr_of_meldingen_keuringsinstantie_kenteken_x_meldingen_keuringsinstantie_meld_datum_door_keuringsinstantie_dt_': f'nr_of_{key}'}, inplace=True)\n",
    "\n",
    "# drop those that didn't need to go long\n",
    "api_cols = [re.match('^(.*)_[0-9]+$',c)[1] for c in df0.columns if c.startswith(f'{key}_api_')]\n",
    "api_cols = np.unique(api_cols)\n",
    "for col in api_cols:\n",
    "    sel = df0.columns.str.startswith(col)\n",
    "    rm_cols = df0.columns[sel]\n",
    "    df0.loc[:, re.sub(f'{key}_', '', col)] = df0.loc[:, sel].ffill(axis=1).T.drop_duplicates().T.iloc[:,0]\n",
    "    df0.drop(columns=rm_cols, inplace=True)\n",
    "\n",
    "print(df0.shape)\n",
    "rdw_per_reg[key] = df0\n",
    "if VERBOSE > 1:\n",
    "    display(rdw_per_reg[key])\n",
    "else:\n",
    "    print('\\n'.join(rdw_per_reg.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Query other available apis\n",
    "for api_name in ['api_gebrek_constateringen']: # columns in 'meldingen_keuringsinstantie'\n",
    "    print(api_name, end=': ')\n",
    "    key = re.sub('^api_','',api_name)\n",
    "    for api_url in rdw_per_reg['meldingen_keuringsinstantie'][api_name].unique():\n",
    "        if ~api_url.endswith('.json'):\n",
    "            # translate\n",
    "            code = [t for t in api_url.split('/') if len(t)>0][-1]\n",
    "            api_url = f'https://opendata.rdw.nl/resource/{code}.json'\n",
    "        print(api_url)\n",
    "\n",
    "        # query the web\n",
    "        q = get_query_url(api_url,regs)\n",
    "        df0 = pd.read_json(q).add_prefix(key + '_')\n",
    "        # name of index\n",
    "        df0.columns.name = api_name\n",
    "\n",
    "        # query should return 'kenteken', make it the index\n",
    "        if df0.shape[0] != 0:\n",
    "            df0.set_index(key + '_kenteken', inplace=True)\n",
    "\n",
    "        # add to list\n",
    "        print(df0.shape)\n",
    "        rdw_per_reg[key]=df0\n",
    "\n",
    "# Add description that is in different api\n",
    "api_name = 'api_gebrek_beschrijving'\n",
    "print(api_name, end=': ')\n",
    "key = re.sub('^api_','',api_name)\n",
    "api_url = rdw_per_reg['meldingen_keuringsinstantie'][api_name].unique()\n",
    "assert len(api_url) == 1\n",
    "api_url = api_url[0]\n",
    "if ~api_url.endswith('.json'):\n",
    "    # translate\n",
    "    code = [t for t in api_url.split('/') if len(t)>0][-1]\n",
    "    api_url = f'https://opendata.rdw.nl/resource/{code}.json'\n",
    "    print(api_url)\n",
    "\n",
    "# query the web if there are registrations\n",
    "if rdw_per_reg['gebrek_constateringen'].shape[0] > 0:\n",
    "    q = get_query_url(api_url, \n",
    "                      rdw_per_reg['gebrek_constateringen'].gebrek_constateringen_gebrek_identificatie.unique(), \n",
    "                      query_field='gebrek_identificatie')\n",
    "    df0 = pd.read_json(q).add_prefix('gebrek_constateringen_')\n",
    "    # name of index\n",
    "    df0.columns.name = api_name\n",
    "\n",
    "    # query should return 'gebrek_identificatie', make it the index\n",
    "    if df0.shape[0] != 0:\n",
    "        df0.set_index('gebrek_constateringen_gebrek_identificatie', inplace=True)\n",
    "    # Join description\n",
    "    rdw_per_reg['gebrek_constateringen'] = rdw_per_reg['gebrek_constateringen'].join(df0, on='gebrek_constateringen_gebrek_identificatie')\n",
    "\n",
    "    # make wide\n",
    "    long = rdw_per_reg['gebrek_constateringen'].copy()\n",
    "    long.loc[:, 'gebrek_constateringen_meld_datum_door_keuringsinstantie_dt_'] = pd.to_datetime(long.gebrek_constateringen_meld_datum_door_keuringsinstantie_dt)\n",
    "    rdw_per_reg['gebrek_constateringen'] = long_to_wide(long, 'gebrek_constateringen_meld_datum_door_keuringsinstantie_dt_')\n",
    "    rdw_per_reg['gebrek_constateringen'].rename(columns={'nr_of_gebrek_constateringen_kenteken_x_gebrek_constateringen_meld_datum_door_keuringsinstantie_dt_': f'nr_of_gebrek_constateringen'}, inplace=True)\n",
    "\n",
    "    print(rdw_per_reg['gebrek_constateringen'].shape)\n",
    "\n",
    "if VERBOSE > 1:\n",
    "    display(rdw_per_reg['gebrek_constateringen'])\n",
    "else:\n",
    "    print('\\n'.join(rdw_per_reg.keys()))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# .. and some more registrations.\n",
    "for api_name, api_url in zip(['api_toegevoegde_objecten', 'api_keuringen'], \n",
    "                             ['https://opendata.rdw.nl/resource/sghb-dzxx.json', 'https://opendata.rdw.nl/resource/vkij-7mwc.json']):\n",
    "\n",
    "    print(api_name, end=': ')\n",
    "    print(api_url)\n",
    "\n",
    "    # Query api\n",
    "    key = re.sub('^api_','', api_name) # store with this key\n",
    "    q = get_query_url(api_url,regs)\n",
    "    df0 = pd.read_json(q).add_prefix(key + '_')\n",
    "    print(df0.shape)\n",
    "    if df0.shape[0] == 0:\n",
    "        print('NO RESULTS')\n",
    "        continue\n",
    "    df0.set_index(key + '_kenteken', inplace=True)\n",
    "\n",
    "\n",
    "    # make wide\n",
    "    if key == 'toegevoegde_objecten':\n",
    "        df0 = long_to_wide(df0.copy(), key + '_montagedatum')\n",
    "        df0.rename(columns={'nr_of_toegevoegde_objecten_kenteken_x_toegevoegde_objecten_montagedatum': f'nr_of_{key}'}, inplace=True)\n",
    "\n",
    "    rdw_per_reg[key] = df0\n",
    "    if VERBOSE > 1:\n",
    "        display(rdw_per_reg[key])\n",
    "    else:\n",
    "        print('\\n'.join(rdw_per_reg.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "file_name = f'../data/auctions/enriched-results/rdw-reg/rdw-reg-apk-0-data-{DATE}-{month_counter}.pkl'\n",
    "\n",
    "out = pd.concat({k:rdw_per_reg[k] for k in rdw_per_reg.keys() if k not in done_keys}, axis=1)\n",
    "out['TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')\n",
    "\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')\n",
    "\n",
    "done_keys = list(rdw_per_reg.keys())\n",
    "done_keys.pop(0);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Are columns unique?\n",
    "col_names = []\n",
    "for n in [list(c.columns) for c in rdw_per_reg.values()]:\n",
    "    col_names += n \n",
    "if not pd.Series(col_names).is_unique:\n",
    "    display(pd.Series(col_names).value_counts().to_frame().rename(columns={0: 'occurance'}).query('occurance > 1').sort_index())\n",
    "    raise LookupError('Dataframes share column names. Add a suffix to column names might solve this.')\n",
    "\n",
    "# Merge dataframes from different apis\n",
    "df_regs = pd.concat(rdw_per_reg.values(), axis='columns', sort=False)\n",
    "# add timestamp\n",
    "df_regs['TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')\n",
    "# set lot id as index\n",
    "df_regs.index.name = 'kenteken'\n",
    "df_regs = df_regs.reset_index().set_index('lot_index')\n",
    "assert df_regs.kenteken.is_unique, 'Index <kenteken> is not unique.'\n",
    "if VERBOSE > 1:\n",
    "    display(df_regs)\n",
    "else:\n",
    "    print(df_regs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "file_name = f'../data/auctions/enriched-results/rdw-reg/rdw-reg-full-0-data-{DATE}-{month_counter}.pkl'\n",
    "\n",
    "out = df_regs\n",
    "out['TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')\n",
    "\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#rdw_top\" id='rdw_confcodes'><font size=+1><center>^^ TOP ^^</center></font></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformity codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# empty dict\n",
    "rdw_per_confcode = dict()\n",
    "# Conformity codes have sub-divisions. Four fields make a super key\n",
    "key = 'conformity_codes'\n",
    "rdw_per_confcode[key] = rdw_per_reg['gekentekende_voertuigen'][[\n",
    "    'typegoedkeuringsnummer', \n",
    "    'uitvoering', \n",
    "    'variant', \n",
    "    'volgnummer_wijziging_eu_typegoedkeuring'\n",
    "]].dropna().drop_duplicates()\n",
    "rdw_per_confcode[key].reset_index(drop=True, inplace=True)\n",
    "rdw_per_confcode[key].volgnummer_wijziging_eu_typegoedkeuring = rdw_per_confcode[key].volgnummer_wijziging_eu_typegoedkeuring.astype(int)\n",
    "print(len(rdw_per_confcode[key]),'conformity codes in this set')\n",
    "\n",
    "if VERBOSE > 1:\n",
    "    display(rdw_per_confcode[key])\n",
    "\n",
    "print(f'{key} occurs more than once.')\n",
    "# replace(1,np.NaN): to drop single occurence\n",
    "display(\\\n",
    "    rdw_per_reg['gekentekende_voertuigen'].loc[:, rdw_per_confcode[key].columns]\\\n",
    "    .reset_index()\\\n",
    "    .groupby([rdw_per_confcode[key].columns[0]])\\\n",
    "    .nunique()\\\n",
    "    .replace(1,np.NaN)\\\n",
    "    .dropna(how='all')\\\n",
    "    .fillna(1)\\\n",
    "    .astype(int)\\\n",
    "    .sort_values(by='kenteken', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# do conformity api and again get other possible apis\n",
    "api_name = 'api_eeg_voertuigtypegoedkeuring'\n",
    "api_url = 'https://opendata.rdw.nl/resource/55kv-xf7m.json'\n",
    "\n",
    "# Query api\n",
    "key = re.sub('^api_','', api_name) # store with this key\n",
    "q = get_query_url(\n",
    "    api_url, \n",
    "    rdw_per_confcode['conformity_codes'].typegoedkeuringsnummer.unique(), # Use long (year with century) version of conformity code\n",
    "    query_field='typegoedkeuringsnummer'\n",
    ")\n",
    "rdw_per_confcode[key] = pd.read_json(q)\n",
    "# more than one conformity code?\n",
    "assert not (rdw_per_confcode[key].groupby('typegoedkeuringsnummer')['typegoedkeuringsnummer'].count() > 1).any()\n",
    "rdw_per_confcode[key].set_index('typegoedkeuringsnummer', inplace=True)\n",
    "\n",
    "# add slightly different keys (year has no century)\n",
    "rdw_per_confcode['conformity_codes'] = rdw_per_confcode['conformity_codes'].merge(\n",
    "    rdw_per_confcode['eeg_voertuigtypegoedkeuring'].eu_type_goedkeuringssleutel, \n",
    "    how='left', \n",
    "    left_on='typegoedkeuringsnummer', \n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "print(api_name, end=': ')\n",
    "if VERBOSE > 1:\n",
    "    display(rdw_per_confcode[key])\n",
    "print(rdw_per_confcode[key].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#rdw_top\" id='rdw_other_apis'><font size=+1><center>^^ TOP ^^</center></font></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _query_all_conf_code_from_api(api_url, l = rdw_per_confcode['conformity_codes'].loc[:,'eu_type_goedkeuringssleutel'].to_list()):\n",
    "    \n",
    "    MAX_ROWS = 50000\n",
    "    \n",
    "    q = api_url\n",
    "    q += '?$$app_token=' + hidden_api_keys.socrata_apptoken \n",
    "    q += '&$where='\n",
    "    l = str(tuple(l))\n",
    "    l = re.sub(' ','',l)\n",
    "    q += urllib.parse.quote(f\"eu_type_goedkeuringssleutel IN {l:s}\")\n",
    "    q += f'&$limit={MAX_ROWS}'\n",
    "    #print(q)\n",
    "    all_conf = pd.read_json(q)\n",
    "\n",
    "    if all_conf.shape[0] == MAX_ROWS:\n",
    "        raise NotImplementedError('Max nr of rows reached.')\n",
    "        \n",
    "    return all_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _query_one_by_one_from_api(api_url, l = rdw_per_confcode['conformity_codes']):\n",
    "    # unfortunately this needs to be done one by one, because conformity code is not unique                   \n",
    "    out = pd.DataFrame()\n",
    "    nr = l.shape[0]\n",
    "    bar = ['.'] * nr\n",
    "    c = -1\n",
    "    disp_id = display({'text/plain': ''.join(bar)}, raw=True, display_id=True)\n",
    "    for ix, row in l.iterrows():\n",
    "        disp_id.update({'text/plain': ''.join(bar)}, raw=True)\n",
    "        c += 1\n",
    "        q = api_url\n",
    "        q += '?$$app_token=' + hidden_api_keys.socrata_apptoken \n",
    "        q += '&{}=\\'{}\\''.format('eu_type_goedkeuringssleutel', urllib.parse.quote(row.eu_type_goedkeuringssleutel))\n",
    "        q += '&{}=\\'{}\\''.format('eeg_uitvoeringscode', urllib.parse.quote(row.uitvoering))\n",
    "        q += '&{}=\\'{}\\''.format('eeg_variantcode', urllib.parse.quote(row.variant))\n",
    "        q += '&{}={:.0f}'.format('uitvoering_wijzigingsnummer', row.volgnummer_wijziging_eu_typegoedkeuring)\n",
    "\n",
    "        n_try = 0\n",
    "        OK = False\n",
    "        while (n_try < 10) & (not OK):\n",
    "            try:\n",
    "                res = pd.read_json(q, \n",
    "                                   dtype={\n",
    "                                       'eu_type_goedkeuringssleutel': str, \n",
    "                                       'eeg_uitvoeringscode': str, \n",
    "                                       'eeg_variantcode': str, \n",
    "                                       'uitvoering_wijzigingsnummer': int\n",
    "                                   })\n",
    "                OK = True\n",
    "                n_try = 0\n",
    "            except:\n",
    "                n_try +=1\n",
    "                sleep(10)\n",
    "            if n_try == 10:\n",
    "                raise 'tried too often'\n",
    "\n",
    "        if len(res) == 0:\n",
    "            bar[c] = 'x'\n",
    "            continue\n",
    "        else:\n",
    "            bar[c] = '|'\n",
    "\n",
    "        res.index=[c] * res.shape[0]   \n",
    "        out = pd.concat([out, res], axis=0)\n",
    "        \n",
    "    disp_id.update({'text/plain': ''.join(bar)}, raw=True)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduce_to_current_set(all_conf, this_set = rdw_per_confcode['conformity_codes'].loc[:,['eu_type_goedkeuringssleutel', 'uitvoering', 'variant', 'volgnummer_wijziging_eu_typegoedkeuring']]):\n",
    "    \n",
    "    idx_left =   ['eu_type_goedkeuringssleutel', 'uitvoering',          'variant',         'volgnummer_wijziging_eu_typegoedkeuring']\n",
    "    idx_right =  ['eu_type_goedkeuringssleutel', 'eeg_uitvoeringscode', 'eeg_variantcode', 'uitvoering_wijzigingsnummer']\n",
    "    \n",
    "    \n",
    "    if \\\n",
    "    (all_conf.shape[0] == 0) \\\n",
    "    or ((this_set.loc[:,idx_left].T.reset_index().T.merge(\n",
    "        all_conf.loc[:,idx_right].T.reset_index().T,\n",
    "        how = 'outer',\n",
    "        on = [0,1,2,3],\n",
    "        indicator=True\n",
    "    )._merge == 'both').any() == False):\n",
    "        \n",
    "        # None are in this_set\n",
    "        out = pd.DataFrame(columns=idx_right) # empty\n",
    "        \n",
    "    else:    \n",
    "        \n",
    "        out = pd.merge(\n",
    "            left = this_set,\n",
    "            right = all_conf,\n",
    "            how = 'inner',\n",
    "            left_on = idx_left,\n",
    "            right_on= idx_right,\n",
    "        ).drop(columns=idx_left[1:]) # drop duplicate left_indices. They are in right_index.\n",
    "    \n",
    "    # progess bar\n",
    "    df_ = rdw_per_confcode['conformity_codes'].rename(columns={l:r for l,r in zip(idx_left,idx_right)})    \n",
    "    df_.set_index(idx_right, inplace=True)\n",
    "    bar = ['|' if i in out.set_index(idx_right).sort_index().index else 'x' for i in df_.index]\n",
    "    print(''.join(bar))\n",
    "              \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _widen_and_add_pfx(res, api_name):\n",
    "\n",
    "    # Set index of input\n",
    "    res.set_index(['eu_type_goedkeuringssleutel', 'eeg_uitvoeringscode', 'eeg_variantcode', 'uitvoering_wijzigingsnummer'], inplace=True)\n",
    "    \n",
    "    key = re.sub('^api_','',api_name)\n",
    "    pfx = re.sub('_eeg_uitvoering$', '', key)\n",
    "    \n",
    "    \n",
    "    # This api has no pivot column, but needs pivoting. Use index\n",
    "    if api_name == 'api_merk_uitvoering_toegestaan':\n",
    "        res['volgnummer_door_index'] = res.groupby(list(res.index.names)).cumcount().add(1)\n",
    "\n",
    "    # Pivot columns. \n",
    "    #     key: api name, \n",
    "    #     value: column name to pivot on. Needs to be a counter.\n",
    "    pivot_columns = {\n",
    "        'api_as_gegevens_eeg_uitvoering': 'asnummer',\n",
    "        'api_handelsbenaming_uitvoering': 'volgnummer',\n",
    "        'api_carrosserie_uitvoering': 'carrosserie_volgnummer',\n",
    "        'api_plaatsaanduiding_uitvoering': 'plaats_aanduiding_volgnummer',\n",
    "        'api_carrosserie_uitvoering_nummerieke_code': 'carrosserie_uitvoering_numeriek_volgnummer',\n",
    "        'api_uitvoeringverbruik_per_uitgave': 'uitvgavenummer_verbruikboek',\n",
    "        'api_motor_uitvoering': 'volgnummer',\n",
    "        'api_versnellingsbak_uitvoering': 'volgnummer',\n",
    "        'api_motor_uitvoering_brandstof': ['volgnummer', 'brandstof_volgnummer'],\n",
    "        'api_merk_uitvoering_toegestaan': 'volgnummer_door_index',\n",
    "    }\n",
    "    \n",
    "    # Get pivot columns according to dict\n",
    "    if api_name in pivot_columns.keys():\n",
    "        piv_col = pivot_columns[api_name]\n",
    "    else:\n",
    "        piv_col = None\n",
    "        \n",
    "    # If input is empty, override by planning to do no modifications\n",
    "    if res.shape[0] == 0:\n",
    "        piv_col = None\n",
    "        api_name = ''\n",
    "\n",
    "    if piv_col is not None:\n",
    "        if isinstance(piv_col, str):\n",
    "            # pivot\n",
    "            piv = res.pivot(columns = piv_col)\n",
    "        elif isinstance(piv_col, list):\n",
    "            piv = pd.pivot_table(res, index = res.index.names, columns = piv_col)\n",
    "        \n",
    "        # flatten column to one level\n",
    "        one_level = ['_'.join([str(c) if type(c)==int else c for c in l]) for l in piv.columns]\n",
    "        piv.columns = one_level\n",
    "    \n",
    "    # Modify when not in dict\n",
    "    # elif api_name == 'api_merk_uitvoering_toegestaan---':\n",
    "    #     # concat all observations in list\n",
    "    #     piv = pd.pivot_table(res, index = res.index.names, values = res.columns, aggfunc = list)\n",
    "        \n",
    "    # No modifications\n",
    "    else: \n",
    "        piv = res\n",
    "    \n",
    "    # sanity check. \n",
    "    assert piv.index.is_unique, f'There are multiple observations per key: [{\", \".join(piv.index.names)}]' # you might want to concat to list, prototyped here above.\n",
    "        \n",
    "    # Add prefix\n",
    "    return piv.add_prefix(pfx+'_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dtype_from_api(api_url):\n",
    "    resp=urllib.request.urlopen(api_url)\n",
    "    hdr = {i[0]:i[1] for i in resp.headers.raw_items()}\n",
    "    dtypes = {f:v for v,f in zip(eval(hdr['X-SODA2-Types']), eval(hdr['X-SODA2-Fields']))}\n",
    "    return dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# query other available apis\n",
    "api_list = [c for c in rdw_per_confcode['eeg_voertuigtypegoedkeuring'].columns if c.startswith('api')]\n",
    "for api_name in api_list:\n",
    "    print(f'{{:{max([len(c) for c in api_list])}s}}'.format(api_name), \n",
    "          end=''.join(['|' if api_name==list_element else '.'  for list_element in api_list]))\n",
    "    key = re.sub('^api_','',api_name)\n",
    "    for api_url in rdw_per_confcode['eeg_voertuigtypegoedkeuring'][api_name].unique():\n",
    "        \n",
    "        # reformat url\n",
    "        M=re.search('https://opendata.rdw.nl/.*/([a-z0-9]{4}-[a-z0-9]{4})$', api_url)\n",
    "        api_url = 'https://opendata.rdw.nl/resource/{}.json'.format(M[1])\n",
    "        print(api_url)\n",
    "       \n",
    "        # query the web\n",
    "        try:\n",
    "            # get all conformation codes in one go.\n",
    "            all_conf = _query_all_conf_code_from_api(api_url) \n",
    "            df1 = _reduce_to_current_set(all_conf)\n",
    "        except NotImplementedError:\n",
    "            df1 = _query_one_by_one_from_api(api_url)\n",
    "\n",
    "        print(df1.shape)\n",
    "        df1 = _widen_and_add_pfx(df1, api_name)\n",
    "        \n",
    "        # add to dict\n",
    "        rdw_per_confcode[key] = df1\n",
    "        if VERBOSE > 1:\n",
    "            display(rdw_per_confcode[key].tail())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#rdw_top\" id='rdw_ovi'><font size=+1><center>^^ TOP ^^</center></font></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed data from rdw (OVI)\n",
    "Optionally get data from rdw website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closed_data(reg, fields = '*'):\n",
    "    web_url = f'https://ovi.rdw.nl/?kenteken={reg}'\n",
    "\n",
    "    import requests\n",
    "    import codecs\n",
    "    from lxml import html, etree\n",
    "    import re\n",
    "    import time\n",
    "    \n",
    "    def _all_fields(tree):\n",
    "        elements = tree.xpath('//*[@class=\"ui-block-d border ovigrid\"]') + tree.xpath('//*/div[@class=\"ui-block-d ovigrid\"]')\n",
    "        return [el.attrib['id'] for el in elements if 'id' in el.keys()]\n",
    "    \n",
    "    # get page\n",
    "    page = requests.get(web_url)\n",
    "    DecodeType = re.findall('charset=(.*)$', page.headers[\"Content-type\"])[0]\n",
    "    htmlstring = codecs.decode(page.content, DecodeType)\n",
    "    # Convert string to tree object\n",
    "    tree = html.fromstring(htmlstring)\n",
    "\n",
    "    # analyze results\n",
    "    ERROR = len(tree.xpath('//*[@action=\"FoutPagina.aspx?error=ErrorBlocked\"]')) == 1\n",
    "    warn_text = tree.xpath('//*[@class=\"warning\"]//text()')\n",
    "    WARNING_NA = (len(warn_text) > 0) and (\n",
    "        (warn_text[0].startswith(f'Er zijn geen gegevens gevonden voor het ingevulde kenteken {reg.upper()}.')) or \\\n",
    "        (warn_text[0].startswith(f'{reg.upper()} is geen geldig kenteken.'))\n",
    "    )\n",
    "\n",
    "    # return result\n",
    "    if ERROR:\n",
    "        # print('X', end='')\n",
    "        return False, 'X'\n",
    "    elif WARNING_NA:\n",
    "        if VERBOSE > 1: print(warn_text[0])\n",
    "        # print('x', end='')\n",
    "        return None, 'x'\n",
    "    else:\n",
    "        if fields == '*':\n",
    "            fields = _all_fields(tree)\n",
    "        out = dict()\n",
    "        succes = [0,0]\n",
    "        for fld in fields:\n",
    "            txt = tree.xpath(f'//*[@id=\"{fld}\"]/text()')\n",
    "            if len(txt) != 1:\n",
    "                # No result with this field. Just skip\n",
    "                if VERBOSE > 1: print(fld)\n",
    "                succes[0] += 1\n",
    "            else:\n",
    "                succes[1] += 1\n",
    "                out[fld] = str(txt[0])\n",
    "        # print(f'[{succes[1]}/{sum(succes)}]', end='')\n",
    "\n",
    "        # No result with these fields. Probably limit request\n",
    "        if all([v is None for v in out.values()]):\n",
    "            raise AssertionError(f'No result with fields: {\", \".join(fields)}.\\n{web_url}')\n",
    "\n",
    "\n",
    "    out['TimeStamp'] = time.strftime('%Y%m%d')\n",
    "    \n",
    "    return out, f'[{succes[1]}/{sum(succes)}]'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prog_mtx(prog):\n",
    "    n_col = 10\n",
    "    prev_r = -1\n",
    "    lines = ''\n",
    "    for n,msg in enumerate(prog.values):\n",
    "        c = n%n_col\n",
    "        r = int((n-c)/n_col)\n",
    "        if prev_r < r:\n",
    "            lines += f'\\n{(prev_r+1)*n_col+1:03.0f}-{(r+1)*n_col:03.0f}'\n",
    "            prev_r = r\n",
    "        lines += f'{msg:>7s}'\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "if CLOSEDDATA:\n",
    "    data = {k: False for k in df_regs.kenteken}\n",
    "    prog = pd.Series(index = data.keys(), data = '-')\n",
    "    disp_id = display({'text/plain': _get_prog_mtx(prog)}, display_id=True, raw=True)\n",
    "    if VERBOSE > 1: print('X: error (site time out after 30 requests)\\nx: reg invalid\\n[x/y]: <x> fields succesfully retrieved from a total of <y> fields\\n-: N.A.')\n",
    "    DO_LOOP = True\n",
    "    c = 0 #counter\n",
    "    max_loop = 2 * len(data) / 30 # approx nr of succesful retrievals is 30. Twice the amount should retrieve all\n",
    "    while DO_LOOP:\n",
    "        \n",
    "        # break\n",
    "        if c > max_loop:\n",
    "            raise RuntimeError(f'Max nr of loops ({c}) reached.')\n",
    "\n",
    "        # pause\n",
    "        if c > 0:\n",
    "            if VERBOSE > 1: print(f' time out (60s) for next iteration', end='')\n",
    "            sleep(60)\n",
    "            if VERBOSE > 1: print(f': {c:2.0f}/{max_loop:2.0f}')\n",
    "        c+=1 \n",
    "\n",
    "        # has no value or field has no value\n",
    "        to_do_regs = [k for k,v in data.items() if v == False]\n",
    "        if VERBOSE > 0: print(f'\\nto do:{len(to_do_regs):3.0f}', end=' ')\n",
    "        for reg in to_do_regs:\n",
    "            res, msg = get_closed_data(reg, closed_data_fields)\n",
    "            prog.loc[reg] = msg\n",
    "            disp_id.update({'text/plain':_get_prog_mtx(prog)}, raw=True)\n",
    "            if msg == 'X':\n",
    "                break\n",
    "            data.update({reg: res})\n",
    "        DO_LOOP = any([v==False for v in data.values()])\n",
    "\n",
    "    # drop None\n",
    "    data = {k:v for k,v in data.items() if v is not None}    \n",
    "    rdw_closed = pd.DataFrame.from_dict(data=data, orient='index')\n",
    "else:\n",
    "    rdw_closed = None\n",
    "\n",
    "if VERBOSE > 1:\n",
    "    rdw_closed\n",
    "else:\n",
    "    print(rdw_closed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#rdw_top\" id='rdw_merge'><font size=+1><center>^^ TOP ^^</center></font></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from The National Highway Traffic Safety Administration (NHTSA)\n",
    "Based on VIN. Product Information Catalog and Vehicle Listing (vPIC)\n",
    "https://vpic.nhtsa.dot.gov/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vin_lookup import Nhtsa_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dictionary\n",
    "nhtsa_per_vin = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "key = 'vpic'\n",
    "df_ =  drz.loc[:, ['Vin', 'Mfyear']].copy() # copy from drz\n",
    "\n",
    "# borrow mfyear from rdw info\n",
    "rdw_myr = pd.merge(  left = rdw_per_reg['registrations'].reset_index(),\n",
    "                     right = rdw_per_reg['gekentekende_voertuigen'].datum_eerste_toelating.reset_index(),\n",
    "                     how='left',\n",
    "                     right_on='kenteken',\n",
    "                     left_on='kenteken'\n",
    "                    ).loc[:, ['lot_index', 'datum_eerste_toelating']].set_index('lot_index')\n",
    "df_ =  pd.concat([df_, (rdw_myr // 10000).astype(pd.Int16Dtype())], axis=1)\n",
    "df_.update(df_.loc[:, ['Mfyear', 'datum_eerste_toelating']].replace({'': np.NaN}).bfill(axis=1))\n",
    "df_.rename(columns={'Vin': 'VIN', 'Mfyear': 'MFY'}, inplace=True)\n",
    "nhtsa_per_vin[key] = df_.loc[:, ['VIN', 'MFY']]\n",
    "\n",
    "# lookup vins in batches\n",
    "Batch = Nhtsa_batch(nhtsa_per_vin[key].iloc[:,:2], verbose=VERBOSE)\n",
    "Batch.full_parse()\n",
    "out = Batch.data.copy()\n",
    "\n",
    "# store in dict\n",
    "nhtsa_per_vin[key] = pd.concat([\n",
    "    nhtsa_per_vin[key],\n",
    "    out.drop(columns=out.columns[out.columns.str.startswith('system') | out.columns.str.startswith('internal')])\n",
    "], axis=1)\n",
    "\n",
    "if VERBOSE > 1:\n",
    "    display(nhtsa_per_vin[key])\n",
    "else:\n",
    "    print('\\n'.join(nhtsa_per_vin.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "file_name = f'../data/auctions/enriched-results/nhtsa-vpic/nhtsa-vpic-0-data-{DATE}-{month_counter}.pkl'\n",
    "\n",
    "out = pd.concat([\n",
    "    nhtsa_per_vin['vpic'].loc[:, ['VIN', 'MFY']], \n",
    "    Batch.data\n",
    "], axis = 1)\n",
    "out['TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')\n",
    "\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets\n",
    "- Merge dataframes from conformity codes apis\n",
    "- Merge with registration results\n",
    "- Merge with auction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# first merge first two results\n",
    "tmp = rdw_per_confcode['conformity_codes'].merge(rdw_per_confcode['eeg_voertuigtypegoedkeuring'], how='left', \n",
    "                                              left_on='typegoedkeuringsnummer',\n",
    "                                              right_index=True\n",
    "                                             )\n",
    "\n",
    "tmp.rename(columns={\n",
    "    'eu_type_goedkeuringssleutel_x': 'eu_type_goedkeuringssleutel',\n",
    "    'uitvoering': 'eeg_uitvoeringscode',\n",
    "    'variant': 'eeg_variantcode',\n",
    "    'volgnummer_wijziging_eu_typegoedkeuring': 'uitvoering_wijzigingsnummer',\n",
    "}, inplace=True)\n",
    "tmp.set_index(['eu_type_goedkeuringssleutel', 'eeg_uitvoeringscode', 'eeg_variantcode', 'uitvoering_wijzigingsnummer'], inplace=True)\n",
    "\n",
    "\n",
    "# list of df. Remove the ones in merged into tmp\n",
    "to_concat = [tmp] + [v for k,v in rdw_per_confcode.items() if k not in ['conformity_codes', 'eeg_voertuigtypegoedkeuring']]\n",
    "# Are columns unique?\n",
    "col_names = []\n",
    "for n in [list(c.columns) for c in to_concat]:\n",
    "    col_names += n \n",
    "if not pd.Series(col_names).is_unique:\n",
    "    display(pd.Series(col_names).value_counts().to_frame().rename(columns={0: 'occurance'}).query('occurance > 1').sort_index())\n",
    "    raise LookupError('Dataframes share column names. Add a suffix to column names might solve this.')\n",
    "    \n",
    "# merge with subsequent api results\n",
    "df_confcodes = pd.concat(to_concat, axis='columns', sort=False)\n",
    "# add timestamp\n",
    "df_confcodes['TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')\n",
    "\n",
    "if VERBOSE > 1:\n",
    "    display(df_confcodes)\n",
    "else:\n",
    "    print(df_confcodes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "file_name = f'../data/auctions/enriched-results/rdw-conf/rdw-conf-0-data-{DATE}-{month_counter}.pkl'\n",
    "\n",
    "out = df_confcodes\n",
    "\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge confirmation codes with registrations\n",
    "df_regs.index.name = 'lot_index'\n",
    "df = df_regs.reset_index().merge(df_confcodes.reset_index(), how='left',\n",
    "                   left_on=['typegoedkeuringsnummer', 'uitvoering', 'variant', 'volgnummer_wijziging_eu_typegoedkeuring'],\n",
    "                   right_on=['typegoedkeuringsnummer', 'eeg_uitvoeringscode', 'eeg_variantcode', 'uitvoering_wijzigingsnummer'],\n",
    ").set_index('lot_index')\n",
    "if VERBOSE > 1:\n",
    "    display(df)\n",
    "else:\n",
    "    print(df.shape)\n",
    "assert all(df.columns.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge closed source data with registrations\n",
    "\n",
    "# make fields lowercase and add \"ovi_\"\n",
    "rdw_closed.index.name='kenteken'\n",
    "rdw_closed.columns = ['ovi' + re.sub(r'([A-Z])',r'_\\1', c).lower() if c != 'TimeStamp' else c for c in rdw_closed.columns ]\n",
    "\n",
    "# Basic operations\n",
    "rdw_closed['ovi_private_owners'] = rdw_closed.ovi_eigenaren.str.split('/').apply(lambda x:int(x[0].strip()))\n",
    "rdw_closed['ovi_company_owner'] = rdw_closed.ovi_eigenaren.str.split('/').apply(lambda x:int(x[1].strip()))\n",
    "rdw_closed['ovi_owners'] = rdw_closed['ovi_private_owners'] + rdw_closed['ovi_company_owner']\n",
    "rdw_closed['ovi_under_survey'] = rdw_closed.ovi_wachten_op_keuring.apply(lambda x: {'Ja': True, 'Nee': False}[x])\n",
    "\n",
    "df = df.merge(rdw_closed, how='left', left_on='kenteken', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "file_name = f'../data/auctions/enriched-results/rdw-ovi/rdw-ovi-0-data-{DATE}-{month_counter}.pkl'\n",
    "\n",
    "out = rdw_closed.merge(df.reset_index().loc[:, ['lot_index', 'kenteken']], how='left', right_on='kenteken', left_index=True).set_index('lot_index')\n",
    "out.rename(columns={'kenteken': 'ovi_kenteken'}, inplace=True)\n",
    "out['ovi_TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')\n",
    "\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO when nhtsa gets more merge them here \n",
    "assert len(nhtsa_per_vin) == 1\n",
    "df_vins = nhtsa_per_vin['vpic']\n",
    "# add timestamp\n",
    "df_vins['TimeStamp'] = pd.Timestamp.now().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge rdw and drz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge auction results and rdw queries. Add prefix \"rdw_\"\n",
    "rdw = pd.concat([drz, df.add_prefix('rdw_')], axis='columns', sort=False)\n",
    "# There should be no duplicates in column names\n",
    "assert all(rdw.columns.value_counts() == 1)\n",
    "# indices should match\n",
    "assert rdw.index.isin(drz.index).all() & drz.index.isin(rdw.index).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "file_name = f'../data/auctions/enriched-results/rdw-reg/rdw-reg-0-data-{DATE}-{month_counter}.pkl'\n",
    "\n",
    "out = rdw.copy()\n",
    "out = out.loc[:, [c for c in out.columns if c.startswith('rdw_') & (not c.startswith('rdw_ovi_'))]]\n",
    "\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge nhtsa and drz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge auction results and rdw queries. Add prefix \"rdw_\"\n",
    "rdw = pd.concat([rdw, df_vins.drop(columns=['VIN', 'MFY']).add_prefix('nhtsa_')], axis='columns', sort=False)\n",
    "# There should be no duplicates in column names\n",
    "assert rdw.columns.is_unique\n",
    "# indices should match\n",
    "assert rdw.index.isin(drz.index).all() & drz.index.isin(rdw.index).all()\n",
    "\n",
    "out = rdw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#rdw_top\" id='rdw_save'><font size=+1><center>^^ TOP ^^</center></font></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_name = file_name.replace('pdf', 'rdw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "file_name = f'../data/auctions/enriched-results/rdw-data-{DATE}-{month_counter}.pkl'\n",
    "if NO_PRICE:\n",
    "    file_name = file_name.replace('.pkl', '-without-price.pkl')\n",
    "if OPBOD:\n",
    "    file_name = file_name.replace('.pkl', '-opbod.pkl')\n",
    "    \n",
    "if (SKIPSAVE==False) and (not(os.path.isfile(file_name))):\n",
    "    print(file_name)\n",
    "    out.to_pickle(file_name)\n",
    "else:\n",
    "    print(f'Skip. {file_name} exists or saving is disabled in settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: download images (or parallel)\n",
    "\n",
    "Because images might be taken down from the drz site, it is advisable to run the notebook that downloads images soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbconvert_instruction:remove_all_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "assert False, 'Stop running. Below is to check if existing file has same dataframe.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdw = pd.read_pickle('../data/rdw-data-2021-06-opzij.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_eq = rdw.eq(out)\n",
    "is_na = rdw.isna() & out.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Raw_text'\n",
    "\n",
    "c = 0\n",
    "for aa,bb in zip(rdw[col],out[col]):\n",
    "    c+=1\n",
    "    sd1 = np.setdiff1d(aa,bb)\n",
    "    sd2 = np.setdiff1d(bb,aa)\n",
    "    if (len(sd1) == 1 & len(sd2) == 1) & (sd2[0][:-1] == sd1[0]):\n",
    "        continue\n",
    "    else:\n",
    "        if sd1:\n",
    "            for l in sd1:\n",
    "                print(f'|{l}|')\n",
    "        if sd2:\n",
    "            for l in sd2:\n",
    "                print(f'|{l}|')\n",
    "        raise\n",
    "            \n",
    "is_eq[col] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'LotCat'\n",
    "\n",
    "c = 0\n",
    "for aa,bb in zip(rdw[col],out[col]):\n",
    "    c+=1\n",
    "    sd1 = np.setdiff1d(aa,bb)\n",
    "    sd2 = np.setdiff1d(bb,aa)\n",
    "    if (len(sd1) == 1 & len(sd2) == 1) & (sd2[0][:-1] == sd1[0]):\n",
    "        continue\n",
    "    else:\n",
    "        if sd1:\n",
    "            for l in sd1:\n",
    "                print(f'|{l}|')\n",
    "        if sd2:\n",
    "            for l in sd2:\n",
    "                print(f'|{l}|')\n",
    "        raise\n",
    "            \n",
    "is_eq[col] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = (is_na | is_eq)\n",
    "\n",
    "sel.loc[:, sel.columns.str.startswith('rdw_TimeStamp')] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    pd.concat([rdw.loc[(sel == False).any(axis=1), sel.all() == False]], keys=['on disk'], axis=1),\n",
    "    pd.concat([out.loc[(sel == False).any(axis=1), sel.all() == False]], keys=['memory'], axis=1)\n",
    "], axis = 1).sort_index(level=1, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in rdw.loc[:, sel.all() == False].columns:\n",
    "    print(col)\n",
    "    display(pd.concat([\n",
    "        rdw.loc[sel[col]==False, col], \n",
    "        out.loc[sel[col]==False, col]\n",
    "    ], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('max_rows', 999):\n",
    "    display(out[out.rdw_kenteken == 'J892TZ'].T.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
